# abstract

## purpose
the purpose of the project is to create a tool for creating html reports about a git project, to identify gaps to continuous integration.
The tool is intended to be executed locally, for example by initiating 'npm run is-this-ci ${analysis-branch}'. This means, the user has to have a local clone of the remote origin and the tool would be informed about a branch to analyse.

## analysis scope
From git log, ${analysis-branch}, create a cluster analysis report about authors and continuous integration like behavior in terms of commit frequency.
Example source: 
```
git log origin/master --pretty=format:%H|%an|%ae|%ad|%cd --date=iso
```
1. Continuous Integration means, that at least once a day, your work is integrated into origin/master.
2. the report should just focus on the basic assumption, that if the commits are created and submitted in a frame of 8 hours, this is truely considered CI.
- Let's assume - time between creation and submit of a commit
- < 1h ... super frictionless
- 1h - 4h ... high performant
- 4h - 8h ... still continuous
- >8h ... not continuous integration

## analysis result
- cluster_details: dominant_delay_bucket, daily_integration_band, author, commit, daily_integration_rate_percent, max_day_gap
- cluster_summary: dominant_delay_bucket, daily_integration_band, authors, commits
- overall_buckets: bucket, count, percent
- top10_-_authors: author, commits, ci_within_8h_percent, daily_integration_rate_percent, max_day_gap

## glossary
each table and each column should contain tooltip information, corresponding to the glossary:

| result table | column | what the value represents | how the value has been analysed |
|---|---|---|---|
| overall_buckets | bucket | Delay category between author date and committer date | Predefined bins: `<1h`, `1-4h`, `4-8h`, `>8h` based on (committer date − author date) per commit |
| overall_buckets | count | Number of commits in this delay bucket | Count of commits whose delay falls into the bucket |
| overall_buckets | percent | Share of commits in this bucket | `count / total_commits * 100`, rounded to one decimal |
| top10_authors | author | Author identity in git log | Taken from `%an <%ae>` in `git log origin/master` |
| top10_authors | commits | Total commits by that author | Count of commits with matching author identity |
| top10_authors | ci_within_8h_percent | % of that author’s commits integrated within 8 hours | `(commits with delay < 8h) / commits * 100` |
| top10_authors | daily_integration_rate_percent | % of consecutive active-day gaps that are <= 1 day | Build sorted unique commit dates per author (committer date), compute day gaps, then `gaps<=1 / total_gaps * 100` |
| top10_authors | max_day_gap | Longest gap (in days) between consecutive active days | Max of the day-gap series per author |
| cluster_summary | dominant_delay_bucket | Most common delay bucket for authors in the cluster | For each author, pick the delay bucket with the highest commit count; used to label cluster |
| cluster_summary | daily_integration_band | Daily integration rate band for the cluster | Predefined bands on per-author daily integration rate: `daily>=70%`, `daily 50-69%`, `daily 30-49%`, `daily<30%` |
| cluster_summary | authors | Number of authors in the cluster | Count of authors whose dominant bucket and daily band match the cluster |
| cluster_summary | commits | Total commits contributed by authors in the cluster | Sum of commit counts for authors in the cluster |
| cluster_details | dominant_delay_bucket | Dominant delay bucket for the author | Most frequent delay bucket in that author’s commits |
| cluster_details | daily_integration_band | Daily integration band for the author | Band based on that author’s daily integration rate |
| cluster_details | author | Author identity in git log | Taken from `%an <%ae>` in `git log origin/master` |
| cluster_details | commits | Total commits by that author | Count of commits with matching author identity |
| cluster_details | daily_integration_rate_percent | Daily integration rate for the author | See “daily_integration_rate_percent” in top10 table; same calculation |
| cluster_details | max_day_gap | Longest gap between consecutive active days for the author | Max gap in days between consecutive commit dates (committer date) for that author |

## visualization
Table and Graph visualizations for every report, tooltips referring to glossary for the table and column names.
- Graph visualization via graph.js
- Results represented as static html page
- Single page result

The style should orientate on https://www.smarter-software.com.

## tooling
Basically from the perspective of tools and frameworks, a minimal set of tools should be used, preferrably just a git repository as source of analysis and results and report html in the current directory.

## language
A feasible language should be used, I was thinking of typescript executive, but choose which seems the best choice for accomplishing the task.

## programming paradigm
The implementation must strictly follow TDD approach and observable behavior must strictly be implemented using BDD style acceptance tests, following a 4 layer architecture:
- layer 1: the test = an executable specification of the behavior to be achieved, utlizing strictly only constructs - actions and assertions - defined in layer 2. Layer 1 uses Layer 2.
- layer 2: DSL - a domain specific language based on actions and assertions oriented at actions and data in glossary tables and attributes. Layer 2 specifies the DSL.
- layer 3: Protocol driver layer - translates from DSL to the git repository and resulting html report files, whatsoever. Layer 3 implements Layer 2.
- layer 4: The system under test, in our case the git repository that is aim for analysis and html report and supporting files that are used for generating the outcome.

In terms of the 4 layer architecture for bdd style acceptance tests, we achieve modular, loosely coupled, cohesive tests - tests that firmly only depend on DSL and during execution use the protocol driver that translates to implementation detail.

Hence all production code should be created test driven, to ensure sustainable quality. The changes should be small and iteratively made. In case of success, good, in case of unsuccessful test result, the change should be reverted and a new, fresh, different attempt might be better. 100% of all tests need to pass on any change in production code, or the change is considered unsuccessful.

Never must production code and test be changed simultaniously.

Acceptance tests must describe upfront what the system should achieve. We use a 4 layer architecture.

What benefit do we have by following that paradigm: Even if a test fails, the Test is valid as long as our assumptions about the problem we try to solve proof correct.

## first task
Please describe the overall goal we are trying to achieve here, define a programming language and tools we would need if we would follow your suggestion immediately and define a set of ATDD tests to achieve the analysis goals for 'analysis result', 'glossary' and 'visualization'.

# chats response
Here’s a clear starting point you can execute on immediately: a concise goal, a concrete language/tooling choice, and a set of ATDD tests aligned to your analysis result, glossary, and visualization requirements.

Overall goal
Build a local CLI tool that analyzes a git branch (via git log), computes CI‑related metrics (delay buckets and daily integration behavior), and outputs a single static HTML report with tables and graphs. The report must include glossary-based tooltips for every table and column, and visual styling aligned with smarter-software.com.

Recommended language and tools

Language: TypeScript (Node.js). It’s ideal for CLI parsing, HTML generation, test frameworks, and structured data modeling.
Runtime: Node.js (LTS)
Test runner: Jest (or Vitest) for acceptance tests and DSL (no unit tests yet per your TDD/ATDD sequence)
BDD/DSL: Custom DSL in TypeScript (Layer 2), exercised by Jest test files (Layer 1)
Git access: git log executed via child_process (Layer 3)
HTML/Charts: Vanilla HTML + Chart.js (per requirement “graph.js”; use Chart.js unless you meant another library)
Output: single HTML file + small embedded JS/CSS in same file (or adjacent static assets in cwd)
ATDD tests (Layer 1) you can write first
Each test uses only the DSL actions/assertions (Layer 2). The protocol driver (Layer 3) will map them to git log and HTML parsing.

Acceptance Suite: analysis result

Computes overall delay buckets from git log
Given a fixture git repo with commits that span all delay ranges
When I run analysis for branch origin/master
Then the report contains overall_buckets with counts matching expected commits per bucket
And overall_buckets.percent values are correct with one decimal
Computes top10 authors table
Given a fixture repo with >10 authors and known commit distribution
When I run analysis
Then top10_authors includes exactly 10 rows with correct author names and commit counts
And ci_within_8h_percent is computed per author
And daily_integration_rate_percent and max_day_gap match expected day-gap calculations
Computes cluster_details per author
Given a fixture repo with at least 3 authors with distinct behavior
When I run analysis
Then cluster_details contains an entry for each author
And each entry has correct dominant_delay_bucket, daily_integration_band, commits, daily_integration_rate_percent, max_day_gap
Computes cluster_summary aggregation
Given the same fixture repo
When I run analysis
Then cluster_summary includes one row per unique (dominant_delay_bucket, daily_integration_band)
And authors equals the number of authors in each cluster
And commits equals the sum of commits for those authors
Acceptance Suite: glossary tooltips

Tooltips exist for every table and column
Given any report generated
When I inspect the HTML
Then each result table has a tooltip describing its meaning
And each column header has a tooltip that matches the glossary definition for that column
Tooltip content matches glossary (spot-check)
Given the report
When I inspect top10_authors.ci_within_8h_percent header tooltip
Then it equals “(commits with delay < 8h) / commits * 100” (as per glossary)
And similar spot-checks for at least one column per table
Acceptance Suite: visualization

Graph exists for each report table
Given a report generated
When I inspect the HTML
Then each of: overall_buckets, top10_authors, cluster_summary, cluster_details has an associated chart element
And each chart has a data series that matches the corresponding table data
Single-page static HTML output
Given a report generated
When I inspect output
Then there is exactly one HTML page as the report
And it loads without external network requests (no remote assets)
Styling aligned with smarter-software.com
Given a report generated
When I inspect CSS
Then typography, colors, and layout reflect the referenced style guide
And the report is readable on mobile and desktop
Suggested next steps

I can draft the Layer 2 DSL verbs and assertions so the tests are executable.
I can also scaffold the repo with Jest + a first acceptance test for overall_buckets using a minimal fixture repo.
Once you confirm, I’ll proceed test-first and implement the protocol driver after tests exist.
If you want, tell me whether you prefer Jest or Vitest, and which chart library you meant by “graph.js.”

# next steps - these next steps please
1. Scaffold the application setup according to your plan
2. I prefer Vitest
3. Chart library: https://www.chartjs.org/

# chats response
Scaffolded a minimal TypeScript + Vitest setup with a CLI entrypoint, ready for ATDD layers and Chart.js integration later.

Details

Added Node/TS/Vitest tooling config in package.json, tsconfig.json, and vitest.config.ts.
Created a placeholder CLI entry in src/cli.ts that validates the branch argument.
Set up the test layer directories plus a short guide in tests/README.md.
Added a basic .gitignore.
Files added

package.json
tsconfig.json
vitest.config.ts
src/cli.ts
.gitignore
tests/README.md
Next steps you might want

Run npm install to pull dev dependencies.
Run npm run is-this-ci -- origin/master to confirm CLI wiring.
Tell me to start with the first ATDD test (overall buckets) and I’ll draft Layer 1 + Layer 2 only.

# next steps
I performed these steps:
1. ```npm install```
2. ```npm audit fix --force```
3. ```npm run is-this-ci -- origin/main```

Please to start with the first ATDD test (overall buckets) and I’ll draft Layer 1 + Layer 2 only.

# chats response
Added the first ATDD acceptance test (overall buckets) in Layer 1 and a minimal Layer 2 DSL interface that the test calls. No production code or driver logic yet, per the TDD/ATDD sequence.

Files added

tests/acceptance/overall-buckets.spec.ts
tests/dsl/ci-report.dsl.ts
Notes

The acceptance test uses a fixture name fixtures/basic-overall-buckets and expects 8 commits total, evenly distributed across the 4 delay buckets.
The DSL methods throw “not implemented” so the test will fail until we implement Layer 3.
Next steps you might want

Tell me to define the fixture repo structure in Layer 3, or adjust the expected bucket counts/percentages before we implement the driver.
